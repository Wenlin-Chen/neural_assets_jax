{
  "cells": [
    {
      "metadata": {
        "id": "sxx-aH1cZzgX"
      },
      "cell_type": "markdown",
      "source": [
        "Copyright 2024 DeepMind Technologies Limited\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "\n",
        "     https://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8K_e58H8S9d"
      },
      "source": [
        "# Neural Assets MOVi Example\n",
        "\n",
        "This notebook shows how we can train a Neural Assets model on the public MOVi\n",
        "dataset. It does not implement the entire training process such as pre-trained\n",
        "weight loading and optimizer settings.\n",
        "\n",
        "Instead, it helps to understand the input \u0026 output pipeline and 3D control\n",
        "interface of our model.\n",
        "\n",
        "We will load a batch of data, apply data pre-processing, visualize them, and\n",
        "run the model on it to compute losses. We borrow the diffusion implementation\n",
        "from Hugging Face Diffusers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3gfBGlIrCkBf"
      },
      "outputs": [],
      "source": [
        "#@title Imports\n",
        "\n",
        "import diffusion\n",
        "from etils.lazy_imports import *\n",
        "import modules\n",
        "import preprocessing\n",
        "import viz_utils\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SJVrIfyHnkOK"
      },
      "outputs": [],
      "source": [
        "#@title Build the RAW MOVi Dataset\n",
        "\n",
        "# @markdown Dataset settings:\n",
        "VARIANT = 'e'  # @param {type:\"string\"}\n",
        "RESOLUTION = 256  # @param {type:\"integer\"}\n",
        "BATCH_SIZE = 4  # @param {type:\"integer\"}\n",
        "\n",
        "\n",
        "def _get_max_obj_num(variant):\n",
        "  \"\"\"Max number of objects in the dataset.\"\"\"\n",
        "  if variant in ['a', 'b', 'c']:\n",
        "    return 10\n",
        "  elif variant in ['e', 'f']:\n",
        "    return 23\n",
        "  else:\n",
        "    raise ValueError(f'Invalid MOVi variant: {variant}')\n",
        "\n",
        "\n",
        "def load_movi():\n",
        "  \"\"\"Build the MOVi dataset.\"\"\"\n",
        "  ds_name = f'movi_{VARIANT}/{RESOLUTION}x{RESOLUTION}:1.0.0'\n",
        "  ds_builder = tfds.builder(ds_name, data_dir='gs://kubric-public/tfds')\n",
        "  ds = ds_builder.as_dataset(split='train', shuffle_files=False)\n",
        "  ds_iter = iter(ds)\n",
        "  data = next(ds_iter)\n",
        "  return ds, data\n",
        "\n",
        "\n",
        "# We will save the visualization results under this directory\n",
        "save_path = f'./viz/movi_{VARIANT}/'\n",
        "\n",
        "train_ds, sample = load_movi()\n",
        "tf_bboxes_3d = sample['instances']['bboxes_3d']  # [N, T, 8, 3]\n",
        "tf_bboxes_3d = einops.rearrange(tf_bboxes_3d, 'n t ... -\u003e t n ...')\n",
        "bboxes_3d = tf_bboxes_3d.numpy()  # [T, N, 8, 3]\n",
        "\n",
        "video_w_bbox_3d = viz_utils.show_3d_bbox_on_image(\n",
        "    viz_utils.to_numpy(sample['video']),\n",
        "    bboxes_3d=bboxes_3d,\n",
        "    cameras=viz_utils.to_numpy(sample['camera']),\n",
        "    bboxes_center_3d=bboxes_3d.mean(-2),\n",
        ")\n",
        "print('Projected 3D bboxes and centers to 2D video frames')\n",
        "viz_utils.show_video(\n",
        "    video_w_bbox_3d,\n",
        "    fps=8,\n",
        "    codec='gif',\n",
        "    save_path=os.path.join(save_path, 'video_bbox_3d.gif'),\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kdxb0hSLPRwH"
      },
      "outputs": [],
      "source": [
        "#@title Apply Our Data Pre-processing Pipeline and Visualize them\n",
        "\n",
        "preproc_fn = lambda x: preprocessing.preprocess_gv_movi_example(\n",
        "    x,\n",
        "    max_instances=_get_max_obj_num(variant=VARIANT),\n",
        "    resolution=RESOLUTION,\n",
        "    drop_cond_prob=0.1,\n",
        ")\n",
        "train_loader = train_ds.map(preproc_fn).batch(batch_size=BATCH_SIZE)\n",
        "train_loader_iter = iter(train_loader)\n",
        "batch = next(train_loader_iter)\n",
        "batch = viz_utils.to_numpy(batch)\n",
        "\n",
        "print('Loaded training data batch:')\n",
        "print(etree.spec_like(batch))\n",
        "\n",
        "vis_3d_args = dict(\n",
        "    focal_length_lst=batch.get('camera_focal_length', None),\n",
        "    sensor_width_lst=batch.get('camera_sensor_width', None),\n",
        "    camera2image_lst=batch.get('camera_projection', None),\n",
        "    is_proj_4_corner=True,\n",
        "    has_background_bbox=True,\n",
        ")\n",
        "\n",
        "print('\\nSource Images with 2D bboxes (used for appearance token extraction)')\n",
        "viz_utils.draw_bbox(\n",
        "    batch['src_image'],\n",
        "    batch['src_bboxes'],\n",
        "    save_path=os.path.join(save_path, 'src_bbox_2d.png'),\n",
        ")\n",
        "print('Source Images with 3D bboxes (not used in training)')\n",
        "viz_utils.draw_bbox_3d(\n",
        "    batch['src_image'],\n",
        "    batch['src_bboxes_3d'],\n",
        "    save_path=os.path.join(save_path, 'src_bbox_3d.png'),\n",
        "    **vis_3d_args,\n",
        ")\n",
        "print('Background images (used for background token extraction)')\n",
        "viz_utils.show_images(\n",
        "    batch['src_bg_image'], save_path=os.path.join(save_path, 'src_bg.png')\n",
        ")\n",
        "\n",
        "print('\\nTarget Images with 2D bboxes (not used for training)')\n",
        "viz_utils.draw_bbox(\n",
        "    batch['tgt_image'],\n",
        "    batch['tgt_bboxes'],\n",
        "    save_path=os.path.join(save_path, 'tgt_bbox_2d.png'),\n",
        ")\n",
        "print('Target Images with 3D bboxes (used for pose token extraction and the diffusion reconstruction target)')\n",
        "viz_utils.draw_bbox_3d(\n",
        "    batch['tgt_image'],\n",
        "    batch['tgt_bboxes_3d'],\n",
        "    save_path=os.path.join(save_path, 'tgt_bbox_3d.png'),\n",
        "    **vis_3d_args,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TuuHYP3dOW45"
      },
      "outputs": [],
      "source": [
        "#@title Build the Neural Assets Model\n",
        "\n",
        "# We use SD v2.1 as our base generator\n",
        "model_name = 'stable_diffusion_v2_1'\n",
        "generator = diffusion.DiffuserDiffusionWrapper(model_name=model_name)\n",
        "hidden_size = 1024  # the cross-attention dim in the denoising U-Net\n",
        "\n",
        "# Learnable appearance tokens + pose tokens\n",
        "token_dim = hidden_size // 2\n",
        "# We will do RoIAlign to extract 2x2 feature maps as object appearance tokens\n",
        "roi_align_size = 2\n",
        "# We use DINO as our visual encoder\n",
        "dino_version, dino_variant = 'v1', 'B/8'\n",
        "conditioning_encoder = modules.ConditioningEncoder(\n",
        "    appearance_encoder=modules.RoIAlignAppearanceEncoder(\n",
        "        # +1 because we add a global background bbox\n",
        "        shape=(_get_max_obj_num(variant=VARIANT) + 1, token_dim),\n",
        "        image_backbone=modules.DINOViT.from_variant_str(\n",
        "            version=dino_version,\n",
        "            variant=dino_variant,\n",
        "            in_vrange=(0, 1),\n",
        "            use_imagenet_value_range=True,\n",
        "            frozen_model=False,  # we fine-tune DINO\n",
        "        ),\n",
        "        roi_align_size=roi_align_size,  # feature map size: (28, 28)\n",
        "        aggregate_method='flatten',  # flatten the 2x2 feature map\n",
        "    ),\n",
        "    # Treat 3D bbox as object pose\n",
        "    object_pose_encoder=modules.MLPPoseEncoder(\n",
        "        mlp_module=modules.MLP(\n",
        "            hidden_size=token_dim * 2,\n",
        "            output_size=token_dim,\n",
        "            num_hidden_layers=1,\n",
        "        ),\n",
        "        # Duplicate bbox tokens to match the length of appearance tokens\n",
        "        duplicate_factor=roi_align_size**2,\n",
        "    ),\n",
        "    # Mask out background pixels when encoding object tokens\n",
        "    mask_out_bg_for_appearance=True,\n",
        "    background_value=0.5,\n",
        "    # Map the relative camera pose with a MLP\n",
        "    # This serves as the pose token for the background\n",
        "    background_pos_enc_type='mlp',\n",
        ")\n",
        "# Fuse appearance and pose tokens with a neck module\n",
        "conditioning_neck = modules.FeedForwardNeck(\n",
        "    feed_forward_module=nn.Dense(hidden_size),\n",
        ")\n",
        "\n",
        "# Full Model\n",
        "ns_model = modules.ControllableGenerator(\n",
        "    generator=generator,\n",
        "    conditioning_encoder=conditioning_encoder,\n",
        "    conditioning_neck=conditioning_neck,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yPaWTUM0FL36"
      },
      "outputs": [],
      "source": [
        "#@title Model Forward \u0026 Loss Computation\n",
        "\n",
        "input_dict = {\n",
        "    'tgt_images': batch['tgt_image'],\n",
        "    'tgt_object_poses': batch['tgt_bboxes_3d'],\n",
        "    'src_images': batch['src_image'],\n",
        "    'src_bboxes': batch['src_bboxes'],\n",
        "    'src_bg_images': batch['src_bg_image'],\n",
        "}\n",
        "output_dict, params = ns_model.init_with_output(jax.random.key(0), **input_dict)\n",
        "# Compute the denoising loss\n",
        "loss = (output_dict['diff'] - output_dict['pred_diff']) ** 2\n",
        "loss = loss.mean()\n",
        "print('Training loss: ', loss)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "csTLqGTUPuAn"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [
        {
          "file_id": "1ZYL6nDmJCvzc70qi5rwIQ7BSj9a2sdoL",
          "timestamp": 1667473449566
        },
        {
          "file_id": "1rOmaBHyQAVa6NY3nfsM1yba8GC0a8duM",
          "timestamp": 1667402678487
        },
        {
          "file_id": "1bHb2szUEMLP2gKErlnxrigutZyhZ4reh",
          "timestamp": 1667386305836
        },
        {
          "file_id": "1jrPkT2OlRGUpImN2cVetBtQ9PgFelNgy",
          "timestamp": 1665137721073
        },
        {
          "file_id": "144LaPYCJpaCSUX6jMoampo0dTMICcXWV",
          "timestamp": 1640626024159
        },
        {
          "file_id": "1B31Dl0yzZlW22l-X7qSR5S4w1nOHPAg0",
          "timestamp": 1640104963946
        },
        {
          "file_id": "1MXdt8a59t0hLm7Q1Prem3kdHpnixMOeJ",
          "timestamp": 1639481266768
        },
        {
          "file_id": "14OG1REDMq3i9phUABzuSWg3nQhUD_ozK",
          "timestamp": 1639156624846
        }
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
